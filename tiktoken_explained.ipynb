{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details of tiktoken, Explained\n",
    "\n",
    "People who have used OpenAI's GPT series services through API calls are definitely familiar with `tiktoken`. Before making the API call, we can use `tiktoken` to calculate the expected number of tokens we will consume, which helps us estimate the cost (or avoid exceeding the input length limit of the GPT series models).\n",
    "Most people only use `tiktoken` to calculate the number of tokens at a basic level, as shown in the following simple usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "text = \"hello world\"\n",
    "number_of_tokens = len(enc.encode(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article provides an introduction to tiktoken, starting from the basics and gradually delving into some of its details.\n",
    "\n",
    "## There are only five types of Tokenizers in tiktoken.\n",
    "\n",
    "Most people use the function `enc = tiktoken.encoding_for_model(\"gpt-4\")` to find the corresponding tokenizer based on the model name. Currently, OpenAI has over 30 models available, including commonly used ones such as text-davinci-003, gpt-3.5-turbo, gpt-4, and so on.\n",
    "\n",
    "However, by examining the `model.py` file in `tiktoken`, we can see that there are actually only five types of tokenizers: `cl100k_base`, `p50k_base`, `r50k_base`, `p50k_edit`, and `gpt2`. The specific mapping table can be seen in the code below. The parameter used in `encoding_for_model` is first used for exact matching, and if not found, it is converted to a prefix matching. The familiar gpt-4 series models we know actually use `cl100k_base`, which has approximately `100,000` tokens. The earlier text-davinci-003 (gpt-3 series) uses `p50k_base`/`r50k_base`, which have around `50,000` tokens in their vocabulary.\n",
    "\n",
    "```python\n",
    "MODEL_PREFIX_TO_ENCODING: dict[str, str] = {\n",
    "    # chat\n",
    "    \"gpt-4-\": \"cl100k_base\",  # e.g., gpt-4-0314, etc., plus gpt-4-32k\n",
    "    \"gpt-3.5-turbo-\": \"cl100k_base\",  # e.g, gpt-3.5-turbo-0301, -0401, etc.\n",
    "}\n",
    "\n",
    "MODEL_TO_ENCODING: dict[str, str] = {\n",
    "    # chat\n",
    "    \"gpt-4\": \"cl100k_base\",\n",
    "    \"gpt-3.5-turbo\": \"cl100k_base\",\n",
    "    # text\n",
    "    \"text-davinci-003\": \"p50k_base\",\n",
    "    \"text-davinci-002\": \"p50k_base\",\n",
    "    \"text-davinci-001\": \"r50k_base\",\n",
    "    \"text-curie-001\": \"r50k_base\",\n",
    "    \"text-babbage-001\": \"r50k_base\",\n",
    "    \"text-ada-001\": \"r50k_base\",\n",
    "    \"davinci\": \"r50k_base\",\n",
    "    \"curie\": \"r50k_base\",\n",
    "    \"babbage\": \"r50k_base\",\n",
    "    \"ada\": \"r50k_base\",\n",
    "    # code\n",
    "    \"code-davinci-002\": \"p50k_base\",\n",
    "    \"code-davinci-001\": \"p50k_base\",\n",
    "    \"code-cushman-002\": \"p50k_base\",\n",
    "    \"code-cushman-001\": \"p50k_base\",\n",
    "    \"davinci-codex\": \"p50k_base\",\n",
    "    \"cushman-codex\": \"p50k_base\",\n",
    "    # edit\n",
    "    \"text-davinci-edit-001\": \"p50k_edit\",\n",
    "    \"code-davinci-edit-001\": \"p50k_edit\",\n",
    "    # embeddings\n",
    "    \"text-embedding-ada-002\": \"cl100k_base\",\n",
    "    # old embeddings\n",
    "    \"text-similarity-davinci-001\": \"r50k_base\",\n",
    "    \"text-similarity-curie-001\": \"r50k_base\",\n",
    "    \"text-similarity-babbage-001\": \"r50k_base\",\n",
    "    \"text-similarity-ada-001\": \"r50k_base\",\n",
    "    \"text-search-davinci-doc-001\": \"r50k_base\",\n",
    "    \"text-search-curie-doc-001\": \"r50k_base\",\n",
    "    \"text-search-babbage-doc-001\": \"r50k_base\",\n",
    "    \"text-search-ada-doc-001\": \"r50k_base\",\n",
    "    \"code-search-babbage-code-001\": \"r50k_base\",\n",
    "    \"code-search-ada-code-001\": \"r50k_base\",\n",
    "    # open source\n",
    "    \"gpt2\": \"gpt2\",\n",
    "}\n",
    "```\n",
    "\n",
    "So, if you know the name of the Tokenizer, you can directly construct it, for example: `enc = tiktoken.get_encoding(\"cl100k_base\")`.\n",
    "\n",
    "## Attributes of Tokenizer and Hidden Tokens\n",
    "\n",
    "The `Tokenizer` object has several hidden attributes: (According to convention, the variable name for the `Tokenizer` is `enc`.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<|fim_middle|>', '<|endofprompt|>', '<|fim_prefix|>', '<|fim_suffix|>', '<|endoftext|>'}\n",
      "100277\n"
     ]
    }
   ],
   "source": [
    "print(enc.special_tokens_set) # all the special token\n",
    "tokens = enc.token_byte_values() # all the vocabulary (excluding special tokens)\n",
    "print(enc.n_vocab) #size of vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, there are only two types of tokens: regular tokens and special tokens. Therefore, in general, the size of `n_vocab` is the sum of regular tokens and special tokens. However, through experimentation, I found that `cl100k_base` is an exception. It has `100,277` tokens (i.e., the `n_vocab` attribute), but only `100,256` regular tokens and `5` special tokens. In other words, OpenAI has hidden at least `16` special tokens.\n",
    "\n",
    "## Model File of the Tokenizer\n",
    "\n",
    "How are hidden tokens discovered? Here we can see the following code snippet in the `tiktoken_ext/openai_public.py` file:\n",
    "```python\n",
    "def cl100k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\"\n",
    "    )\n",
    "    special_tokens = {\n",
    "        ENDOFTEXT: 100257,\n",
    "        FIM_PREFIX: 100258,\n",
    "        FIM_MIDDLE: 100259,\n",
    "        FIM_SUFFIX: 100260,\n",
    "        ENDOFPROMPT: 100276,\n",
    "    }\n",
    "    return {\n",
    "        \"name\": \"cl100k_base\",\n",
    "        \"pat_str\": r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "```\n",
    "\n",
    "In other words, the model file for the `cl100k_base` tokenizer is stored at this URL (this indicates that the tiktoken package does not include the tokenizer model file and it will be downloaded when called, which can get stuck if the network environment is poor). Based on this, five special tokens are added.\n",
    "\n",
    "The specific content of this file format (with the `.tiktoken` extension) is the base64 encoding of the character sequence of tokens. Please refer to these two functions in `tiktoken/load.py` for more information:\n",
    "\n",
    "```python\n",
    "def dump_tiktoken_bpe(bpe_ranks: dict[bytes, int], tiktoken_bpe_file: str) -> None:\n",
    "    try:\n",
    "        import blobfile\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"blobfile is not installed. Please install it by running `pip install blobfile`.\"\n",
    "        ) from e\n",
    "    with blobfile.BlobFile(tiktoken_bpe_file, \"wb\") as f:\n",
    "        for token, rank in sorted(bpe_ranks.items(), key=lambda x: x[1]):\n",
    "            f.write(base64.b64encode(token) + b\" \" + str(rank).encode() + b\"\\n\")\n",
    "\n",
    "\n",
    "def load_tiktoken_bpe(tiktoken_bpe_file: str) -> dict[bytes, int]:\n",
    "    # NB: do not add caching to this function\n",
    "    contents = read_file_cached(tiktoken_bpe_file)\n",
    "    return {\n",
    "        base64.b64decode(token): int(rank)\n",
    "        for token, rank in (line.split() for line in contents.splitlines() if line)\n",
    "    }\n",
    "```\n",
    "\n",
    "There is a problem here: the indices of special tokens (for the `cl100k_base` tokenizer) are not consecutive. It jumps from 100260 directly to 100276, hiding 16 special tokens in between.\n",
    "\n",
    "The official documentation also validates our idea, as mentioned in its README, that the Tokenizer can be extended in this way:\n",
    "\n",
    "```python\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# In production, load the arguments directly instead of accessing private attributes\n",
    "# See openai_public.py for examples of arguments for specific encodings\n",
    "enc = tiktoken.Encoding(\n",
    "    # If you're changing the set of special tokens, make sure to use a different name\n",
    "    # It should be clear from the name what behaviour to expect.\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        \"<|im_start|>\": 100264,\n",
    "        \"<|im_end|>\": 100265,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "This should be a specific Tokenizer used internally by OpenAI, from which we can deduce the following: Token with index 100264 is used to mark the boundary of user input during the ChatGPT dialogue, while token with index 100265 is used to mark the boundary of machine input (refer to the [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) format for more details).\n",
    "\n",
    "As for the remaining 14 special tokens, we are not aware of their specific purposes. However, based on our speculation, some of them might be related to image inputs (such as marking the beginning and end of an image), while others could be associated with the dialogue roles, such as the user/assistant/system roles in the ChatGPT API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protection of special tokens (to prevent injection attacks)\n",
    "\n",
    "The aforementioned special tokens are typically used to trigger specific capabilities such as image comprehension and model instructions, so user input needs to be prohibited. This is specifically implemented within the encode function.\n",
    "\n",
    "```python\n",
    "    def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        *,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(), \n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = \"all\",\n",
    "    ) -> list[int]:\n",
    "```\n",
    "\n",
    "For each special token, there are three possible processing methods:\n",
    "\n",
    "- Trigger a specific capability of the model as a special token.\n",
    "- Parse it as regular text.\n",
    "- Throw an error.\n",
    "\n",
    "`allowed_special` refers to the special tokens that are allowed to be parsed, while `disallowed_special` refers to the tokens that will result in an error. By default, special tokens are not allowed to be parsed, and they will all result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.encode('<|endoftext|>') # error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when we attempted to use the Tokenizer provided by the OpenAI website, this sentence was parsed as regular text.\n",
    "\n",
    "![Image](https://github-production-user-asset-6210df.s3.amazonaws.com/23236638/242950647-6c8a231a-b3d8-4fa9-b363-fdd597dfe51e.png)\n",
    "\n",
    "Therefore, for text inputs in user calls, OpenAI should be using the parsing method `enc.encode(text, disallowed_special=set())`.\n",
    "As a result, it is not possible for users to insert special tokens in OpenAI API calls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to deal with UNK/OOV\n",
    "\n",
    "A few years ago, when NLP development was not as mature, Tokenizers primarily relied on dictionaries, often resulting in out-of-vocabulary (OOV) situations during the Tokenization phase, where the token could only be marked as an unknown word (UNK).\n",
    "\n",
    "Now, OpenAI's `tiktoken`, based on the byte-pair encoding (BPE) tokenization method, completely avoids the OOV/UNK problem. This is because OpenAI's tiktoken treats the input sequence as a byte sequence and retains all single-byte tokens in its vocabulary, allowing it to handle worst-case scenarios at the character level.\n",
    "\n",
    "The BPE tokenization method essentially involves prefix matching during the tokenization phase, allowing us to construct this prefix tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_tree(lst, enc):\n",
    "    tree = defaultdict(dict)\n",
    "    for item in lst:\n",
    "        current = tree\n",
    "        for byte in item:\n",
    "            if byte not in current:\n",
    "                current[byte] = {}\n",
    "            current = current[byte]\n",
    "        current['end'] = enc._mergeable_ranks[item]\n",
    "    return tree\n",
    "\n",
    "tree = build_tree(enc.token_byte_values(), enc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree has a corresponding token for each byte (within the range of 0-256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256):\n",
    "    assert 'end' in tree[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, by using BPE and incorporating all characters into the vocabulary, we will no longer encounter OOV/UNK issues."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Sentences by tiktoken\n",
    "\n",
    "Those familiar with BERT know that BERT adds a `[CLS]` token at the beginning of a sentence and also marks the start and end of each word (e.g., `##est` indicates the last three characters of a word). However, OpenAI's tiktoken does not perform these preprocessing steps.\n",
    "\n",
    "Whether a character appears at the beginning or end of a word does not affect the token, as shown in the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[359, 359]\n",
      "[359]\n"
     ]
    }
   ],
   "source": [
    "print(enc.encode('unun')) # [359, 359]\n",
    "print(enc.encode('un')) # [359]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the initial \"un\" and the final \"un\" are both the same token, 359. Additionally, there are no special tokens like `[CLS]` added at the beginning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the pat_str\n",
    "\n",
    "In each Tokenizer definition within `tiktoken`, we can see a string of peculiar `pat_str`. Its full name is \"pattern string,\" which is a regular expression.\n",
    "\n",
    "```python\n",
    "def cl100k_base():\n",
    "    # omitted\n",
    "    return {\n",
    "        \"name\": \"cl100k_base\",\n",
    "        \"pat_str\": r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "```\n",
    "\n",
    "The core logic of `tiktoken` is written in Rust. The core code for its encoding is as follows:\n",
    "```rust\n",
    "    fn _encode_ordinary_native(&self, text: &str) -> Vec<usize> {\n",
    "        // This is the core of the encoding logic; the other functions in here\n",
    "        // just make things complicated :-)\n",
    "        let regex = self._get_tl_regex();\n",
    "        let mut ret = vec![];\n",
    "        for mat in regex.find_iter(text) {\n",
    "            let piece = mat.unwrap().as_str().as_bytes();\n",
    "            if let Some(token) = self.encoder.get(piece) {\n",
    "                ret.push(*token);\n",
    "                continue;\n",
    "            }\n",
    "            ret.extend(&byte_pair_encode(piece, &self.encoder));\n",
    "        }\n",
    "        ret\n",
    "    }\n",
    "```\n",
    "\n",
    "Among them, the `regex` variable is derived from the compilation of `pat_str`. Therefore, we have a rough idea of the core logic of `tiktoken`: segmenting the input based on `pat_str` and then tokenizing each segment. Perhaps this is more accurate than simply dividing by spaces?\n",
    "\n",
    "I couldn't find any documentation on how this `pat_str` is constructed. The regular expression is long and difficult to understand. The documentation for the `find_iter` function in Rust can be found at [Regex in fancy_regex](https://docs.rs/fancy-regex/latest/fancy_regex/struct.Regex.html#method.find_iter). Interested friends can take a look.\n",
    "\n",
    "I tried writing a Rust code to test:\n",
    "\n",
    "```rust\n",
    "use fancy_regex::Regex;\n",
    "\n",
    "fn main() {\n",
    "    let re = Regex::new(r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\").unwrap();\n",
    "    let s = \"This is a sentence about hello world\";\n",
    "    for mat in re.find_iter(s) {\n",
    "        println!(\"Matched: {}\", mat.unwrap().as_str());\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Results are:\n",
    "```\n",
    "Matched: This\n",
    "Matched:  is\n",
    "Matched:  a\n",
    "Matched:  sentence\n",
    "Matched:  about\n",
    "Matched:  hello\n",
    "Matched:  world\n",
    "```\n",
    "\n",
    "As seen, the input sentence is segmented into several smaller segments, which are then tokenized using BPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regarding multithreading and parallelism\n",
    "\n",
    "The tokenization process is generally sequential, where the calculation of subsequent tokens depends on the results of the previous ones.\n",
    "\n",
    "However, on the `tiktoken` GitHub repository, there is a figure indicating a result achieved through multithreading and parallel processing:\n",
    "\n",
    "![image](https://github-production-user-asset-6210df.s3.amazonaws.com/23236638/242954855-ef6af119-f812-437e-b64d-8bed119e5fef.png)\n",
    "\n",
    "After carefully reading the code, I found that the characteristics of multithreading only appear in the `encode_batch` function. In other words, the multiprocessing feature is only enabled when tokenizing multiple sentences at once.\n",
    "\n",
    "Here are the timing results I obtained in IPython. It can be observed that when tokenizing less than 8 sentences simultaneously, the time taken is roughly equivalent to tokenizing a single sentence. However, when tokenizing 16 sentences simultaneously, the time taken is approximately twice as long. This is because the default setting uses 8 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'nonsense' * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.7 ms ± 832 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 out = enc.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.6 ms ± 888 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 out = enc.encode_batch([test, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.4 ms ± 1.69 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 out = enc.encode_batch([test] * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.4 ms ± 2.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 out = enc.encode_batch([test] * 16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific parallel details involve the `pyo3` package in the Rust language, allowing multi-threading parallelism through `py.allow_threads`. For more information, please refer to [Parallelism](https://pyo3.rs/v0.13.2/parallelism)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "For a detailed explanation of the specific principles behind byte-pair encoding and an introduction to various tokenization techniques, please refer to the following blog posts:\n",
    "\n",
    "- https://www.freecodecamp.org/news/evolution-of-tokenization/\n",
    "- https://www.freecodecamp.org/news/train-algorithms-from-scratch-with-hugging-face/\n",
    "- https://zhuanlan.zhihu.com/p/631463712 (chinese)\n",
    "- https://zhuanlan.zhihu.com/p/86965595 (chinese)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
